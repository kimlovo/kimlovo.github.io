
<html>
<head>
  <meta charset="UTF-8">
  <title>Audio samples from "Speaker Generation"</title>

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
  <link rel="stylesheet" type="text/css" href="../../stylesheet.css"/>
  <!-- TODO(daisy): finalize stylesheets -->
  <!-- https://google.github.io/tacotron/stylesheet.css -->
  <link rel="stylesheet" type="text/css" href="localstyle.css"/>
  <link rel="shortcut icon" href="../../images/taco.png"/>

  <script src="index.js"></script>
</head>

<body>
  <div id="container">
    <div id="playerTitle"></div>
    <audio controls="" id="player"></audio>
    <div id="debug"></div>
  </div>
  <article>
    <header>
      <h1>Audio samples from "Speaker Generation"</h1>
    </header>
  </article>
  <!-- TODO(daisy): replace with arxiv link -->
  <!-- <div><p><b>Paper:</b> <a href="https://arxiv.org/abs/2111.05095">arXiv</a></p></div> -->
  <!-- <div><p><b>Authors:</b> Daisy Stanton, Matt Shannon, Soroosh Mariooryad, RJ Skerry-Ryan, Eric Battenberg, Tom Bagby, and David Kao</p></div> -->
<!--   <div><p><b>Abstract:</b> This work explores the task of synthesizing speech in non-existent human-sounding voices. We call this task "speaker generation", and present TacoSpawn, a system that performs competitively at this task. TacoSpawn is a recurrent attention-based text-to-speech model that learns a distribution over a speaker embedding space, which enables sampling of novel and diverse speakers. Our method is easy to implement, and does not require transfer learning from speaker ID systems. We present objective and subjective metrics for evaluating performance on this task, and demonstrate that our proposed objective metrics correlate with human perception of speaker similarity.</p></div> -->

<!--   <p>This page contains a set of audio samples in support of the paper: we suggest that readers listen to the samples in conjunction with reading the paper.</p> -->

  <p><b>Model discription:</b> The TTS model is multispeaker FastSpeech2 trained on LibriTTS. It uses 16-dim lookup table as the speaker embedding. After that, EM algorithm is used to train a GMM for speaker generation task.</p>

<!--   <p><a href="../../index.html">Click here for more from the Tacotron team.</a></p>


  <p class="toc_title">Contents</p>
  <div id="toc_container">
  <ul>
    <li><a href="#generated-speakers"> 1. Novel speakers</a></li>
    <li><a href="#speaker-distance"> 2. Speaker distance</a></li>
    <li><a href="#tsne-demo"> 3. Interactive t-SNE plot</a></li>
  </ul>
  </div> -->


<div class="section-break"></div>

<div id="content_body">
<hr>


<div class="section-break"></div>


<a name="tsne-demo"><h2>Interactive t-SNE plot</h2></a>

<p>The interactive t-SNE plot below shows speakers represented in d-vector space. Each audio sample is from either a speaker from ground truth in the dataset or a speaker that is sampled from the GMM. <p>

  <div id="plotlyDiv" style="width: 100%; height: 90%"><!-- Plotly chart will be drawn inside this DIV --></div>
  <audio id='audioDiv' controls>
  </audio>
  <div id="scriptDiv"><!-- Script ID will be written in this DIV --></div>
  <script src="https://cdn.plot.ly/plotly-2.4.2.min.js"></script>
  <script src="./tsne_en1468.js"></script>


</body>
</html>
